<!doctype html><html lang=en><head><title>[Distributed] 分散式訓練介紹 - Kaka's Blog
</title><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no,minimal-ui"><meta name=renderer content="webkit"><meta http-equiv=Cache-Control content="no-transform"><meta http-equiv=Cache-Control content="no-siteapp"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="black"><meta name=format-detection content="telephone=no,email=no,adress=no"><meta name=theme-color content="#000000"><meta http-equiv=window-target content="_top"><meta name=description content="Distributed Training (分散式訓練) - 介紹 分散式訓練是指將訓練模型的工作負載分散到多台機器的多個 GPU 上進行。 分散式訓練 - 平行處理的方式主要分為兩種類型: 資料平行"><meta name=generator content="Hugo 0.126.3 with theme pure"><title>[Distributed] 分散式訓練介紹 - Kaka's Blog</title>
<link rel=stylesheet href=https://kaka-lin.github.io/my-blog/css/style.min.c42838cb837ce405361b2d6e3a1e99ddcc6be6d8be2766f2fe32a151d5664f19.css><link rel=stylesheet href=https://kaka-lin.github.io/my-blog/css/custom.css async><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/styles/atom-one-dark.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.css async><meta property="og:url" content="https://kaka-lin.github.io/my-blog/2024/09/introduction/"><meta property="og:site_name" content="Kaka's Blog"><meta property="og:title" content="[Distributed] 分散式訓練介紹"><meta property="og:description" content="Distributed Training (分散式訓練) - 介紹 分散式訓練是指將訓練模型的工作負載分散到多台機器的多個 GPU 上進行。 分散式訓練 - 平行處理的方式主要分為兩種類型: 資料平行"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-10T00:00:00+00:00"><meta property="article:modified_time" content="2024-09-10T00:00:00+00:00"><meta property="article:tag" content="Distributed"><meta property="article:tag" content="PyTorch"><meta property="og:see_also" content="https://kaka-lin.github.io/my-blog/2024/06/segmentation/"><meta property="og:see_also" content="https://kaka-lin.github.io/my-blog/2022/09/resize_image/"><meta property="og:see_also" content="https://kaka-lin.github.io/my-blog/2022/09/progress_bar/"><meta property="og:see_also" content="https://kaka-lin.github.io/my-blog/2022/09/custom_training_loop/"><meta property="og:see_also" content="https://kaka-lin.github.io/my-blog/2022/08/introduction/"><meta property="og:see_also" content="https://kaka-lin.github.io/my-blog/2022/08/progress_bar/"><meta property="og:see_also" content="https://kaka-lin.github.io/my-blog/2022/08/progress_bar/"><meta property="og:see_also" content="https://kaka-lin.github.io/my-blog/2022/08/datasets_dataloaders/"><meta property="og:see_also" content="https://kaka-lin.github.io/my-blog/2022/08/tensors/"><meta itemprop=name content="[Distributed] 分散式訓練介紹"><meta itemprop=description content="Distributed Training (分散式訓練) - 介紹 分散式訓練是指將訓練模型的工作負載分散到多台機器的多個 GPU 上進行。 分散式訓練 - 平行處理的方式主要分為兩種類型: 資料平行"><meta itemprop=datePublished content="2024-09-10T00:00:00+00:00"><meta itemprop=dateModified content="2024-09-10T00:00:00+00:00"><meta itemprop=wordCount content="3583"><meta itemprop=keywords content="Distributed,PyTorch"><meta name=twitter:card content="summary"><meta name=twitter:title content="[Distributed] 分散式訓練介紹"><meta name=twitter:description content="Distributed Training (分散式訓練) - 介紹 分散式訓練是指將訓練模型的工作負載分散到多台機器的多個 GPU 上進行。 分散式訓練 - 平行處理的方式主要分為兩種類型: 資料平行"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body class="main-center theme-black" itemscope itemtype=http://schema.org/WebPage><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=slimContent><div class=navbar-header><div class="profile-block text-center"><a id=avatar href=https://github.com/kaka-lin target=_blank><img class="img-circle img-rotate" src=https://kaka-lin.github.io/my-blog/images/kaka_virtual.jpg width=200 height=200></a><h2 id=name class="hidden-xs hidden-sm">Kaka Lin</h2><h3 id=title class="hidden-xs hidden-sm hidden-md">Software Engineer</h3><small id=location class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i>Taipei, Taiwan</small></div><div class=search id=search-form-wrap><form class="search-form sidebar-form"><div class=input-group><input type=text class="search-form-input form-control" placeholder=Search>
<span class=input-group-btn><button type=submit class="search-form-submit btn btn-flat" onclick=return!1><i class="icon icon-search"></i></button></span></div><div class=ins-search><div class=ins-search-mask></div><div class=ins-search-container><div class=ins-input-wrapper><input type=text class=ins-search-input placeholder="Type something..." x-webkit-speech>
<button type=button class="close ins-close ins-selectable" data-dismiss=modal aria-label=Close><span aria-hidden=true>×</span></button></div><div class=ins-section-wrapper><div class=ins-section-container></div></div></div></div></form></div><button class="navbar-toggle collapsed" type=button data-toggle=collapse data-target=#main-navbar aria-controls=main-navbar aria-expanded=false>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span></button></div><nav id=main-navbar class="collapse navbar-collapse" itemscope itemtype=http://schema.org/SiteNavigationElement role=navigation><ul class="nav navbar-nav main-nav"><li class="menu-item menu-item-home"><a href=/my-blog/><i class="icon icon-home-fill"></i>
<span class=menu-title>Home</span></a></li><li class="menu-item menu-item-categories"><a href=/my-blog/categories/><i class="icon icon-folder"></i>
<span class=menu-title>Categories</span></a></li><li class="menu-item menu-item-books"><a href=/my-blog/series/><i class="icon icon-book-fill"></i>
<span class=menu-title>Study Notes</span></a></li><li class="menu-item menu-item-tags"><a href=/my-blog/tags/><i class="icon icon-tags"></i>
<span class=menu-title>Tags</span></a></li><li class="menu-item menu-item-about"><a href=/my-blog/about/><i class="icon icon-cup-fill"></i>
<span class=menu-title>About</span></a></li></ul></nav></div></header><aside class=sidebar itemscope itemtype=http://schema.org/WPSideBar><div class=slimContent><div class=widget><h3 class=widget-title>Categories</h3><div class=widget-body><ul class=category-list><li class=category-list-item><a href=https://kaka-lin.github.io/my-blog/categories/c++/ class=category-list-link>c++</a><span class=category-list-count>3</span></li><li class=category-list-item><a href=https://kaka-lin.github.io/my-blog/categories/canbus/ class=category-list-link>canbus</a><span class=category-list-count>2</span></li><li class=category-list-item><a href=https://kaka-lin.github.io/my-blog/categories/data-science/ class=category-list-link>data science</a><span class=category-list-count>4</span></li><li class=category-list-item><a href=https://kaka-lin.github.io/my-blog/categories/data-structure-and-algorithm/ class=category-list-link>data structure and algorithm</a><span class=category-list-count>16</span></li><li class=category-list-item><a href=https://kaka-lin.github.io/my-blog/categories/database/ class=category-list-link>database</a><span class=category-list-count>4</span></li><li class=category-list-item><a href=https://kaka-lin.github.io/my-blog/categories/devops/ class=category-list-link>devops</a><span class=category-list-count>2</span></li><li class=category-list-item><a href=https://kaka-lin.github.io/my-blog/categories/docker/ class=category-list-link>docker</a><span class=category-list-count>1</span></li><li class=category-list-item><a href=https://kaka-lin.github.io/my-blog/categories/go/ class=category-list-link>go</a><span class=category-list-count>8</span></li><li class=category-list-item><a href=https://kaka-lin.github.io/my-blog/categories/ml/dl/ class=category-list-link>ml/dl</a><span class=category-list-count>11</span></li><li class=category-list-item><a href=https://kaka-lin.github.io/my-blog/categories/os/ class=category-list-link>os</a><span class=category-list-count>3</span></li><li class=category-list-item><a href=https://kaka-lin.github.io/my-blog/categories/python/ class=category-list-link>python</a><span class=category-list-count>13</span></li><li class=category-list-item><a href=https://kaka-lin.github.io/my-blog/categories/qt/ class=category-list-link>qt</a><span class=category-list-count>1</span></li><li class=category-list-item><a href=https://kaka-lin.github.io/my-blog/categories/react/ class=category-list-link>react</a><span class=category-list-count>2</span></li></ul></div></div><div class=widget><h3 class=widget-title>Series</h3><div class=widget-body><ul class=series-list><li class=series-list-item><a href=https://kaka-lin.github.io/my-blog/series/asynchronous-programming/ class=series-list-link>asynchronous programming</a><span class=series-list-count>7</span></li><li class=series-list-item><a href=https://kaka-lin.github.io/my-blog/series/data-structure-and-algorithm/ class=series-list-link>data structure and algorithm</a><span class=series-list-count>16</span></li><li class=series-list-item><a href=https://kaka-lin.github.io/my-blog/series/distributed/ class=series-list-link>distributed</a><span class=series-list-count>1</span></li><li class=series-list-item><a href=https://kaka-lin.github.io/my-blog/series/go/ class=series-list-link>go</a><span class=series-list-count>7</span></li><li class=series-list-item><a href=https://kaka-lin.github.io/my-blog/series/machine-learning/ class=series-list-link>machine learning</a><span class=series-list-count>11</span></li><li class=series-list-item><a href=https://kaka-lin.github.io/my-blog/series/python/ class=series-list-link>python</a><span class=series-list-count>13</span></li><li class=series-list-item><a href=https://kaka-lin.github.io/my-blog/series/pytorch/ class=series-list-link>pytorch</a><span class=series-list-count>4</span></li><li class=series-list-item><a href=https://kaka-lin.github.io/my-blog/series/react/ class=series-list-link>react</a><span class=series-list-count>2</span></li><li class=series-list-item><a href=https://kaka-lin.github.io/my-blog/series/tensorflow/ class=series-list-link>tensorflow</a><span class=series-list-count>3</span></li><li class=series-list-item><a href=https://kaka-lin.github.io/my-blog/series/yolo-series/ class=series-list-link>yolo series</a><span class=series-list-count>1</span></li></ul></div></div><div class=widget><h3 class=widget-title>Tags</h3><div class=widget-body><ul class=tag-list><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/array/ class=tag-list-link>array</a><span class=tag-list-count>1</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/automatic-differentiation/ class=tag-list-link>automatic differentiation</a><span class=tag-list-count>3</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/canbus/ class=tag-list-link>canbus</a><span class=tag-list-count>2</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/ci/cd/ class=tag-list-link>ci/cd</a><span class=tag-list-count>1</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/classification/ class=tag-list-link>classification</a><span class=tag-list-count>1</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/concurrency/ class=tag-list-link>concurrency</a><span class=tag-list-count>2</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/coroutine/ class=tag-list-link>coroutine</a><span class=tag-list-count>2</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/distributed/ class=tag-list-link>distributed</a><span class=tag-list-count>1</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/docker/ class=tag-list-link>docker</a><span class=tag-list-count>1</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/dp/ class=tag-list-link>dp</a><span class=tag-list-count>2</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/generator/ class=tag-list-link>generator</a><span class=tag-list-count>1</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/google-trends/ class=tag-list-link>google trends</a><span class=tag-list-count>2</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/hash-table/ class=tag-list-link>hash table</a><span class=tag-list-count>1</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/heap/ class=tag-list-link>heap</a><span class=tag-list-count>2</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/i/o-models/ class=tag-list-link>i/o models</a><span class=tag-list-count>1</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/jupyter/ class=tag-list-link>jupyter</a><span class=tag-list-count>1</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/kvaser/ class=tag-list-link>kvaser</a><span class=tag-list-count>1</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/letterbox/ class=tag-list-link>letterbox</a><span class=tag-list-count>1</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/linked-list/ class=tag-list-link>linked list</a><span class=tag-list-count>1</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/matplotlib/ class=tag-list-link>matplotlib</a><span class=tag-list-count>1</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/metrics/ class=tag-list-link>metrics</a><span class=tag-list-count>3</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/mysql/ class=tag-list-link>mysql</a><span class=tag-list-count>1</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/object-detection/ class=tag-list-link>object detection</a><span class=tag-list-count>2</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/oop/ class=tag-list-link>oop</a><span class=tag-list-count>2</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/orm/ class=tag-list-link>orm</a><span class=tag-list-count>1</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/parallelism/ class=tag-list-link>parallelism</a><span class=tag-list-count>1</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/progressbar/ class=tag-list-link>progressbar</a><span class=tag-list-count>2</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/pytorch/ class=tag-list-link>pytorch</a><span class=tag-list-count>1</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/segmentation/ class=tag-list-link>segmentation</a><span class=tag-list-count>1</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/socketcan/ class=tag-list-link>socketcan</a><span class=tag-list-count>2</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/sqlalchemy/ class=tag-list-link>sqlalchemy</a><span class=tag-list-count>1</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/sqlite/ class=tag-list-link>sqlite</a><span class=tag-list-count>1</span></li><li class=tag-list-item><a href=https://kaka-lin.github.io/my-blog/tags/tree/ class=tag-list-link>tree</a><span class=tag-list-count>11</span></li></ul></div></div><div class=widget><h3 class=widget-title>Recent Posts</h3><div class=widget-body><ul class="recent-post-list list-unstyled no-thumbnail"><li><div class=item-inner><p class=item-title><a href=https://kaka-lin.github.io/my-blog/2024/09/introduction/ class=title>[Distributed] 分散式訓練介紹</a></p><p class=item-date><time datetime="2024-09-10 00:00:00 +0000 UTC" itemprop=datePublished>2024-09-10</time></p></div></li><li><div class=item-inner><p class=item-title><a href=https://kaka-lin.github.io/my-blog/2024/06/segmentation/ class=title>[Segmentation] 影像分割指標 (Segmentation Metrics)</a></p><p class=item-date><time datetime="2024-06-04 00:00:00 +0000 UTC" itemprop=datePublished>2024-06-04</time></p></div></li><li><div class=item-inner><p class=item-title><a href=https://kaka-lin.github.io/my-blog/2023/03/socketcan_example/ class=title>[CanBus] Example C code for SocketCAN</a></p><p class=item-date><time datetime="2023-03-27 00:00:00 +0000 UTC" itemprop=datePublished>2023-03-27</time></p></div></li><li><div class=item-inner><p class=item-title><a href=https://kaka-lin.github.io/my-blog/2023/03/kvaser_socketcan/ class=title>[CanBus] SocketCAN Support for Kvaser Devices</a></p><p class=item-date><time datetime="2023-03-23 00:00:00 +0000 UTC" itemprop=datePublished>2023-03-23</time></p></div></li><li><div class=item-inner><p class=item-title><a href=https://kaka-lin.github.io/my-blog/2022/09/red_black_tree_removal/ class=title>[DSA] 紅黑樹 (Red-Black Tree) 介紹 - Part 2: Removal</a></p><p class=item-date><time datetime="2022-09-28 00:00:00 +0000 UTC" itemprop=datePublished>2022-09-28</time></p></div></li></ul></div></div></div></aside><aside class="sidebar sidebar-toc collapse" id=collapseToc itemscope itemtype=http://schema.org/WPSideBar><div class=slimContent><h4 class=toc-title>Catalogue</h4><nav id=toc class="js-toc toc"></nav></div></aside><main class=main role=main><div class=content><article id=- class="article article-type-" itemscope itemtype=http://schema.org/BlogPosting><div class=article-header><h1 itemprop=name><a class=article-title href=/my-blog/2024/09/introduction/>[Distributed] 分散式訓練介紹</a></h1><div class=article-meta><span class=article-date><i class="icon icon-calendar-check"></i>&nbsp;
<a href=https://kaka-lin.github.io/my-blog/2024/09/introduction/ class=article-date><time datetime="2024-09-10 00:00:00 +0000 UTC" itemprop=datePublished>2024-09-10</time>
</a></span><span class=article-category><i class="icon icon-folder"></i>&nbsp;
<a class=article-category-link href=/my-blog/categories/ml/dl/>ML/DL </a></span><span class=article-series><i class="icon icon-book"></i>&nbsp;
<a class=article-series-link href=/my-blog/series/machine-learning/>Machine Learning </a><a class=article-series-link href=/my-blog/series/pytorch/>PyTorch </a><a class=article-series-link href=/my-blog/series/distributed/>Distributed </a></span><span class=article-tag><i class="icon icon-tags"></i>&nbsp;
<a class=article-tag-link href=/my-blog/tags/distributed/>Distributed </a><a class=article-tag-link href=/my-blog/tags/pytorch/>PyTorch </a></span><span class=post-comment><i class="icon icon-comment"></i>&nbsp;<a href=/my-blog/2024/09/introduction/#comments class=article-comment-link>Comments</a></span></div></div><div class="article-entry marked-body js-toc-content" itemprop=articleBody><h1 id=distributed-training-分散式訓練---介紹>Distributed Training (分散式訓練) - 介紹</h1><p>分散式訓練是指將訓練模型的工作負載分散到多台機器的多個 GPU 上進行。</p><p>分散式訓練 - 平行處理的方式主要分為兩種類型:</p><ul><li><a href=#%E8%B3%87%E6%96%99%E5%B9%B3%E8%A1%8C%E8%99%95%E7%90%86-data-parallel>資料平行處理 (Data Parallel)</a></li><li><a href=#%E6%A8%A1%E5%9E%8B%E5%B9%B3%E8%A1%8C%E8%99%95%E7%90%86-model-parallel>模型平行處理 (Model Parallel)</a></li></ul><p>當然也可以結合兩種方法: <a href=#data_plus_model_parallel>資料平行處理 + 模型平行處理</a></p><p>分散式訓練 - 系統架構(調度的方式)主要分為兩種類型:</p><ul><li><a href=#parameter-server>Parameter Server</a></li><li><a href=#ring-all-reduce>Ring-All-Reduce</a></li></ul><h2 id=資料平行處理-data-parallel>資料平行處理 (Data Parallel)</h2><blockquote><p>資料平行處理是實作兩種分散式訓練方法中最簡單的方式，且足以用於大部分的使用案例。</p></blockquote><p>當<code>數據量非常大，並且模型能夠放置在單個 GPU 上時</code>，就可以採用資料平行化的方式來進行。</p><p>資料平行處理作法是，在不同的 GPU 上都複製一份模型，然後依照一些規則將資料分配到不同的 GPU 上進行計算，各自進行訓練後，將所有 GPU 計算的結果合併，再進行參數更新，從而達到加速模型訓練的目的。如下圖所示:</p><p><img src=/my-blog/images/ml/distributed/introduction/images/data_parallel_1.png alt></p><h5 id=example-azure-資料平行處理原則>Example: Azure 資料平行處理原則</h5><p>以下說明 Azure 中的 Data Parallel，如下圖所示:</p><p><img src=/my-blog/images/ml/distributed/introduction/images/data_parallel_2.png alt></p><p>如上，資料會分割成分割區，而分割區數目等於計算叢集中可用節點的總數。 模型會在其中每個背景工作節點中複製，而且每個背景工作會在自己的資料子集上運作。 請記住，每個節點都必須有容量來支援要訓練的模型，也就是模型必須完全符合每個節點。</p><p>每個節點都會個別計算其訓練範例的預測和已標記輸出之間的誤差。 接著，每個節點都會根據誤差來更新其模型，而且必須將其所有變更傳達給其他節點，以更新其對應的模型。 這表示，背景工作節點必須在批次計算結束時同步模型參數或梯度，以確保其訓練一致的模型。</p><h3 id=資料平行處理-參數更新的方式>資料平行處理: 參數更新的方式</h3><p>由於資料平行會涉及到把不同 GPU 的計算結果進行合併然後再進行參數更新，根據跟新方式不同，可以分為:</p><p><code>在資料並行中，每個GPU只計算一個batch中的一部分資料</code></p><ul><li><p>同步更新 (synchronous):</p><p>訓練時會等待所有 GPU 都計算完畢後，再統一更新模型的參數，因此訓練速度上會比使用異步的方式來得慢。但因為在更新參數時會合併其他計算結果，相當於增加了 batch size 的大小，對於訓練結果有一定的提升。</p></li><li><p>非同步、異步更新 (asynchronous):</p><p>每個 GPU 在獨立計算完之後，不須等待其他 GPU，可以各自立即更新參數。能夠提升訓練速度，但可能會產生 <code>Slow and Stale Gradients (梯度失效、梯度過期)</code> 問題，收斂過程較不穩定，訓練結果會比使用同步的方式差。</p><ul><li>梯度失效:每一個 GPU 完成訓練之後，都會馬上去更新參數，這會造成其他 GPU 現在的模型引數和這一輪訓練前採用的模型引數可能不一致，從而導致此時的梯度過期。</li></ul><blockquote><p>非同步更新雖然快，但是由於梯度失效問題，模型往往會陷入到次優解中。</p></blockquote></li></ul><p><img src=/my-blog/images/ml/distributed/introduction/images/data_parallel_method.png alt></p><h2 id=模型平行處理-model-parallel>模型平行處理 (Model Parallel)</h2><blockquote><p>各個 GPU 只加載了模型的一部分網路結構，存在一定的依賴關係，
造成了規模的伸縮性比較差，不能隨意的增減 GPU 的數量，因此在實際中運用的並不多。</p></blockquote><p>當<code>模型架構太大以至於在一個 GPU 放不下時</code>，可以採用<code>模型平行化 (model parallel)</code>的方式來進行。</p><pre><code>model parallel，也叫做 tensor parallel
</code></pre><p>模型平行處理的作法是，將整個神經網路模型拆解並分配到不同的 GPU 上，且不同的 GPU 負責模型中不同的部份。由於模型層與層之間通常有依賴性，也就是指在進行前向傳播、反向傳播時，前面及後面的層會作為彼此的輸入和輸出，因此這種序列的邏輯對加速造成了一定的限制。如下:</p><p><img src=/my-blog/images/ml/distributed/introduction/images/model_parallel_1.png alt></p><p>此方法通常是在模型非常大時，，如: <code>GPT-3</code>，單個 GPU 的記憶體已經完全裝部下整個網路時才會使用。但相比起來，我們可以此方式把一個超大模型訓練起來，不然對於一個單GPU的話，超大模型是完全沒辦法 work 的。</p><p>因此若非使用較大的模型較不建議採用模型平行處理。若想提升訓練速度，可以選擇較容易進行平行運算的 model，如: <code>Inception</code>。</p><h5 id=example-azure-模型平行處理原則>Example: Azure 模型平行處理原則</h5><p>以下說明 Azure 中的 Model Parallel，如下:</p><p>在模型平行處理中 (也稱為網路平行處理)，模型會分割成不同部分，並且可以在不同的節點中同時執行，而且每一個部分都會在相同的資料上執行。 此方法的可擴縮性取決於演算法的工作平行處理程度，而且比資料平行處理的實作更複雜。</p><p>在模型平行處理中，背景工作節點只需要同步共用參數，通常是針對每個向前或向後傳播步驟進行一次同步。 此外，較大的模型也不會有問題，因為每個節點都是在相同訓練資料的模型子區段上運作。</p><h2 id=兩者結合-資料平行處理--模型平行處理>兩者結合: 資料平行處理 + 模型平行處理</h2><p>當然也可以兩種方法: <code>資料平行處理 + 模型平行處理</code>方式，如下圖所示:</p><p><img src=/my-blog/images/ml/distributed/introduction/images/model_and_data_parallel.png alt></p><p>作法是:</p><ul><li><code>模型平行處理</code>: 將模型拆解並分配到單個機器中不同的 GPU 上，</li><li><code>資料平行處理</code>: 同時將模型複製到其他機器中的每個 GPU 中，而資料數據也會依照一些規則分配到不同的 GPU 上</li></ul><h2 id=分散式訓練系統架構>分散式訓練系統架構</h2><p>上面介紹的是平行處理的方式。那這些平行化運算的方式是如何進行加速的呢？各個設備又是如何將計算結果相互交換的呢？分散式訓練之所以能達到這兩件事是藉由系統架構的設計來實現。現在我們來看實際在分散式訓練中是怎麼進行訓練的調度的呢？</p><p>系統架構(調度的方式)主要分為兩種類型:</p><ul><li><a href=#parameter-server>Parameter Server</a></li><li><a href=#ring-all-reduce>Ring-All-Reduce</a></li></ul><h3 id=parameter-server>Parameter Server</h3><blockquote><p>是一種參數集中調度的架構</p></blockquote><p>資料平行訓練產生局部梯度，再匯總梯度後更新模型參數權重。如下圖所示:</p><p><img src=/my-blog/images/ml/distributed/introduction/images/parameter_server_1.png alt></p><blockquote><p>注意: 此方法為<code>同步更新 (synchronous)</code></p></blockquote><h4 id=parameter-server-整體架構>Parameter Server 整體架構</h4><p>Parameter Server 架構主要分為兩大部份，如下圖:</p><p><img src=/my-blog/images/ml/distributed/introduction/images/parameter_server_2.png alt></p><ul><li><p>一個 Parameter Server group:</p><blockquote><p>負責初始化及維護一個完整的全域共享參數(Global Parameters)，將當前的值發送給每個 worker 進行平行訓練。並且接收 worker 傳回的局部梯度，計算全部完整梯度，並更新模型參數。</p></blockquote><p>由 sever manager 和多個 sever node 組成，其中:</p><ul><li><p>sever manager: 負責分配資源並維護各個 sever node 的狀態</p><pre><code>原數據(metadata)的一致性
</code></pre></li><li><p>sever node: 則是負責維護被分配的一部分參數。</p><pre><code>每個 server node 實際上都只負責分到的部分參數
</code></pre></li></ul></li><li><p>多個 worker group:</p><blockquote><p>用於維護從 dataset 分配到的 data 及對應的 sever node 所分配到的部分參數。訊練後獲得的新的梯度會發送到 server group 並更新參數。</p></blockquote><p>每個 worker group 由以下組成:</p><ul><li><p>task scheduler: 負責分配各個 worker node 的任務並進行監控</p></li><li><p>多個 worker node: 負責進行訓練和更新各自分配到的參數</p><pre><code>每個 worker 只分到部分數據和處理任務
</code></pre></li></ul><p>且每個 worker node 只與對應的 sever node 進行通訊，worker group 和 worker group 或 worker group 內部的每個 worker node 之間都不會互相交流。</p></li></ul><h4 id=parameter-server-工作流程---同步更新>Parameter Server 工作流程 - 同步更新</h4><p><img src=images/parameter_server_3.png alt>
<img src=/my-blog/images/ml/distributed/introduction/images/parameter_server_3.png alt></p><p>同步更新流程如上圖所示:</p><ol><li>每個 worker group 會各自進行訓練、計算梯度、更新分配到的部分參數，再將結果傳送給 server node。</li><li>接著 server node 把收到的參數彙總並進行 global 更新 (ex: 平均參數)，再傳回給 worker node。</li></ol><p>由於 server node 需要跟所有對應的 worker node 進行溝通，容易導致 sever node 的計算過重以及網路阻塞的狀況，從而使得訓練速度下降。</p><h5 id=example>Example</h5><p><img src=images/parameter_server_sync_1.gif alt>
<img src=/my-blog/images/ml/distributed/introduction/images/parameter_server_sync_1.gif alt></p><p>上圖中，每個子節點都只維護自己分配到的參數（圖中的黑色），自己部分更新之後，將計算結果（例如：梯度）傳回到主節點，進行全域的更新（比如平均操作之類的），主節點再向子節點傳送新的參數。</p><h4 id=parameter-server-工作流程---異步更新>Parameter Server 工作流程 - 異步更新</h4><p>參數更新可以在 Worker 之間以<code>非同步(Asynchronous)</code>完成，因此 Worker 可以在整個訓練過程中彼此獨立運行，並在大規模訓練中獲得更好的可擴展性，如下圖所示:</p><p><img src=images/parameter_server_async_1.png alt>
<img src=/my-blog/images/ml/distributed/introduction/images/parameter_server_async_1.png alt></p><h5 id=example-1>Example</h5><p><img src=images/parameter_server_async_2.gif alt>
<img src=/my-blog/images/ml/distributed/introduction/images/parameter_server_async_2.gif alt></p><p>比較同步更新如下所示:</p><p><img src=images/parameter_server_sync_2.gif alt>
<img src=/my-blog/images/ml/distributed/introduction/images/parameter_server_sync_2.gif alt></p><h4 id=parameter-server---瓶頸缺點>Parameter Server - 瓶頸/缺點</h4><p>雖然 Parameter Server 的架構容易管理，但隨著 Worker (GPU) 數目增加，maseter (Reducer) 的負擔也會越來越大，如下圖所示:</p><p><img src=images/parameter_server_4.png alt>
<img src=/my-blog/images/ml/distributed/introduction/images/parameter_server_4.png alt></p><p>假設有 5 塊 GPU，每一塊 GPU 擁有完整的模型，總共的訓練 dataset 大小為 K。</p><p>考慮一個簡單的同步更新策略。首先，每張 GPU 擁有同樣的初始參數，我們將大小為 K 的訓練資料分為 N 塊，也就是 5塊，分給每張GPU。每個 GPU 基於自己那一部分的資料，計算得到 local gradients，然後 N-1塊（4塊）GPU 將計算所得的 local gradients 發送給 GPU 0，讓 GPU 0 對所有的 local gradients 進行 reduce(匯聚操作) 得到全域的梯度，然後再將該全域梯度返回給每塊 GPU 來更新每個GPU上的模型參數。</p><p>那麼我們可以計算下整個通信過程的 communication cost:</p><p>假設每張 GPU 需要發送給 GPU 0 的通信資料大小是 1GB，我們的 network bandwidth 是 1GB/s，那麼我們需要 4 秒才可以將數據全部發送到 GPU 0 上，然後計算出 global 的平均梯度。我們可以看到其通信成本是 C * N，由於受 GPU 0 的network bandwidth的影響，<code>通信成本隨著設備數的增加，而線性增長</code>。</p><blockquote><p>Parameter Server 隨著 worker 數量的增加，其 speedup ratio 會急速下降。</p></blockquote><h3 id=ring-all-reduce>Ring-All-Reduce</h3><blockquote><p>與 Parameter Serve 相反，是去中心化調度的架構</p></blockquote><p>Ring-All-Reduce 架構是將所有 GPU 呈現環狀的形式，且每個 GPU 中的數據會分為 N 份，GPU 之間會根據環狀的方式來傳送數據，如下圖所示:</p><p><img src=images/ring_all_reduce_1.jpg alt>
<img src=/my-blog/images/ml/distributed/introduction/images/ring_all_reduce_1.jpg alt></p><p>這樣的架構所產生的通訊量與設備的多寡無關，因此能夠大幅地減少通訊的開銷、更好地平衡設備間通訊的使用程度。</p><p>而且，在訓練過程中，可以利用 <code>Backpropagation 的特性 (後面層的梯度會先於前面層計算)</code>，在計算前面層梯度的同時，進行後面層梯度的傳送，以提升訓練效率。</p><h3 id=ring-all-reduce-工作流程>Ring-All-Reduce 工作流程</h3><p>流程如下所示:</p><ol><li><p>首先第 k 個 GPU 把第 k 份的數據傳給下一個 GPU</p><p><img src=images/ring_all_reduce_2.png alt>
<img src=/my-blog/images/ml/distributed/introduction/images/ring_all_reduce_2.png alt></p></li><li><p>接著下一個 GPU 會將收到的第 k 份數據與自己的第 k 份數據進行加總整合後，再傳送給下一個 GPU。</p><p><img src=images/ring_all_reduce_3.png alt>
<img src=/my-blog/images/ml/distributed/introduction/images/ring_all_reduce_3.png alt></p></li><li><p>經過 N 次的迭代循環後，第 k-1 個 GPU 會收到所有第 k 份的最終整合數據，再以這份數據來更新各自的參數。</p><p><img src=images/ring_all_reduce_4.png alt>
<img src=/my-blog/images/ml/distributed/introduction/images/ring_all_reduce_4.png alt></p></li></ol><p>動畫如下:</p><p><img src=images/ring_all_reduce_5.gif alt>
<img src=/my-blog/images/ml/distributed/introduction/images/ring_all_reduce_5.gif alt></p><h2 id=reference>Reference</h2><ul><li><a href=https://learn.microsoft.com/zh-tw/azure/machine-learning/concept-distributed-training>什麼是分散式訓練? - Azure</a></li><li><a href=https://medium.com/ching-i/pytorch-%E5%88%86%E6%95%A3%E5%BC%8F%E8%A8%93%E7%B7%B4-distributeddataparallel-%E6%A6%82%E5%BF%B5%E7%AF%87-8378e0ead77>Pytorch 分散式訓練 DistributedDataParallel — 概念篇 | by 李謦伊</a></li><li><a href=https://www.gushiciku.cn/pl/gXXt/zh-tw>深度學習中的分散式訓練_OPPO數智技術 - MdEditor</a></li><li><a href=https://speakerdeck.com/yylin1/kubeflow-jin-xing-fen-san-shi-shen-du-xue-xi-xun-lian-zhi-zuo-ye-pai-cheng-ping-jing>Kubeflow 進行分散式深度學習訓練之作業排程瓶頸 - Speaker Deck</a></li><li><a href=https://zhuanlan.zhihu.com/p/101544149>深度学习加速：算法、编译器、体系结构与硬件设计</a></li><li><a href=http://www.yylin.io/2019/01/09/parameter-server-concept/>Parameter Server 學習筆記 - Yi Yang&rsquo;s Blog</a></li><li><a href=https://tech.preferred.jp/en/blog/technologies-behind-distributed-deep-learning-allreduce/>Technologies behind Distributed Deep Learning: AllReduce</a></li><li><a href="https://www.zhihu.com/question/57799212/answer/292494636?utm_source=ZHShareTargetIDMore&amp;utm_medium=social&amp;utm_oi=37729630945280">ring allreduce和tree allreduce的具体区别是什么？ - 知呼</a></li><li><a href=https://www.modb.pro/db/434006>Ring AllReduce简介</a></li><li><a href=https://www.infoq.cn/article/3cuypbqlprdjochsyb2j>浅谈 Tensorflow 分布式架构：ring all-reduce 算法</a></li></ul></div><div class=article-footer><blockquote class=mt-2x><ul class="post-copyright list-unstyled"><li class="post-copyright-link hidden-xs"><strong>Permalink: </strong><a href=https://kaka-lin.github.io/my-blog/2024/09/introduction/ title="[Distributed] 分散式訓練介紹" target=_blank rel=external>https://kaka-lin.github.io/my-blog/2024/09/introduction/</a></li><li class=post-copyright-license><strong>License：</strong><a href=https://creativecommons.org/licenses/by/4.0/deed.zh_TW target=_blank rel=external>CC BY 4.0</a></li></ul></blockquote><div class="panel panel-default panel-badger"><div class=panel-body><figure class=media><div class=media-left><a href=https://github.com/kaka-lin target=_blank class="img-burn thumb-sm visible-lg"><img src=https://kaka-lin.github.io/my-blog/images/kaka_virtual.jpg class="img-rounded w-full" alt></a></div><div class=media-body><h3 class=media-heading><a href=https://github.com/kaka-lin target=_blank><span class=text-dark>Kaka Lin</span><small class=ml-1x>Software Engineer</small></a></h3><div>AIoT</div></div></figure></div></div></div></article><section id=comments><div id=disqus_thread><noscript>Please enable JavaScript to view the <a href=//disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></section></div><nav class="bar bar-footer clearfix" data-stick-bottom><div class=bar-inner><ul class="pager pull-left"><li class=prev><a href=https://kaka-lin.github.io/my-blog/2024/06/segmentation/ title="[Segmentation] 影像分割指標 (Segmentation Metrics)"><i class="icon icon-angle-left" aria-hidden=true></i><span>&nbsp;&nbsp;Older</span></a></li><li class=toggle-toc><a class="toggle-btn collapsed" data-toggle=collapse href=#collapseToc aria-expanded=false title=Catalogue role=button><span>[&nbsp;</span><span>Catalogue</span>
<i class="text-collapsed icon icon-anchor"></i>
<i class="text-in icon icon-close"></i>
<span>]</span></a></li></ul><div class=bar-right></div></div></nav></main><footer class=footer itemscope itemtype=http://schema.org/WPFooter><ul class=social-links><li><a href="https://www.facebook.com/profile.php?id=100001656484042" target=_blank title=facebook data-toggle=tooltip data-placement=top><i class="icon icon-facebook"></i></a></li><li><a href=https://github.com/kaka-lin target=_blank title=github data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li><li><a href=https://www.linkedin.com/in/%E5%AE%B6%E8%B1%AA-%E6%9E%97-014a02116/ target=_blank title=linkedin data-toggle=tooltip data-placement=top><i class="icon icon-linkedin"></i></a></li></ul><div class=copyright>Copyright
&copy;2020 -
2024<div class=publishby>Theme by <a href=https://github.com/xiaoheiAh target=_blank>xiaoheiAh </a>base on<a href=https://github.com/xiaoheiAh/hugo-theme-pure target=_blank> pure</a>.</div></div></footer><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
            showMathMenu: false, //disables context menu
            tex2jax: {
            inlineMath: [ ['$','$'], ['\\(','\\)'] ]
           }
    });
</script><script src=https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js></script><script>window.jQuery||document.write('<script src="js/jquery.min.js"><\/script>')</script><script type=text/javascript src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/highlight.min.js></script><script type=text/javascript src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/languages/python.min.js defer></script><script type=text/javascript src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/languages/javascript.min.js defer></script><script type=text/javascript src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/languages/go.min.js defer></script><script type=text/javascript src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/languages/c.min.js defer></script><script type=text/javascript src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/languages/cpp.min.js defer></script><script>hljs.configure({tabReplace:"    ",classPrefix:""}),hljs.initHighlightingOnLoad()</script><script src=https://kaka-lin.github.io/my-blog/js/application.min.e4989ab4dc212027af8773861b05b6bc333a1217f6b0a1b3377a3a3dbd454483.js></script><script src=https://kaka-lin.github.io/my-blog/js/plugin.min.ee01f0915da127c8f127f6384fd08c677a9a87f16e6b265d282abf291b5065d9.js></script><script>(function(e){var t={TRANSLATION:{POSTS:"Posts",PAGES:"Pages",CATEGORIES:"Categories",TAGS:"Tags",UNTITLED:"(Untitled)"},ROOT_URL:"https://kaka-lin.github.io/my-blog/",CONTENT_URL:"https://kaka-lin.github.io/my-blog//searchindex.json "};e.INSIGHT_CONFIG=t})(window)</script><script type=text/javascript src=https://kaka-lin.github.io/my-blog/js/insight.min.716b0c6a00b68ccc31a2b65345f3412f4246ffa94a90f8e25d525528b4504f9937880692bbe619023233caba5d0a17ebe23d7cfb57cd3a88f23ea337ad5e4d00.js defer></script><script src=https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.min.js></script><script>tocbot.init({tocSelector:".js-toc",contentSelector:".js-toc-content",headingSelector:"h1, h2, h3",hasInnerContainers:!0})</script><script>var disqus_config=function(){this.page.url="https://kaka-lin.github.io/my-blog/2024/09/introduction/",this.page.identifier="kaka-blog"};(function(){var e=document,t=e.createElement("script");t.src="//kaka-blog.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})()</script></body></html>